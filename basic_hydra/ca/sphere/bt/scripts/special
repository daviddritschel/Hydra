#!/usr/bin/env csh

#=========================================================================#
#   Job setup script for the ellipsoidal barotropic class of f90 codes
#         *** Specifically sets up a Bickley jet for testing ***
#=========================================================================#

echo
echo '-----------------------------------------------------------------'
echo ' The ellipsoid-of-revolution barotropic contour advection method '
echo '       *** special script for setting up a Bickley jet ***'
echo '-----------------------------------------------------------------'

# Specify code numerical method class:
set meth="ca"
# Specify code geometry (use "sphere" despite it being an ellipsoid of rev):
set geom="sphere"
# Specify model equation type:
set equa="bt"
# Specify algorithm:
set algo="caps"

#==========================================================================
# The following is totally generic!
set local_home=`homedir`
set hydradir=$local_home/$user/hydra/$meth/$geom/$equa

# Get output base directory:
set workdir=`workdir`
# Make the base data directory if it does not already exist:
if (!(-d $workdir)) mkdir $workdir
set workdir=$workdir/hydra
if (!(-d $workdir)) mkdir $workdir
set basedir=$workdir/$meth
if (!(-d $basedir)) mkdir $basedir
set basedir=$basedir/$geom
if (!(-d $basedir)) mkdir $basedir
set basedir=$basedir/$equa
if (!(-d $basedir)) mkdir $basedir
set basedir=$basedir/$algo
if (!(-d $basedir)) mkdir $basedir

set outdir=$workdir/$meth/$geom/$equa/$algo

# Assign source and binary directories:
set srcdir=$hydradir/$algo
set bindir=$hydradir/scripts

# Default number of bytes representing a single character:
set n_b_p_c=1   #This is the value valid for gfortran

# Get the name of the computer to record in the job_info file below:
set host=`hostname | awk -F. '{print $(1)}'`

#==========================================================================
# generate a random number seed based on current time:
set second=`date +%-S`
set minute=`date +%-M`
set hour=`date +%-H`
set day=`date +%-d`
@ iseed = ( $second + ( 60 * ( $minute + 60 * ( $hour + ( 24 * $day ) ) ) ) )

# Create a temporary directory:
set tmpdir=$outdir/tmp{$iseed}
mkdir $tmpdir
cd $tmpdir

echo
echo ' *** Job will be built in the temporary directory '
echo $tmpdir

# Create a job information summary file:
touch job_info
echo ' Job created at                      ' `date` >> job_info
echo ' on                                  ' $host >> job_info

#==========================================================================
# Set fixed constants:
set pi=3.14159265358979
set twopi=`echo "scale=10; 2 * $pi" | bc -l`

#==========================================================================
# Set defaults for the data generation routine to be used:
set datagen = "bickley"
set forcing = "n"
set damping = "n"
set ng="128"       # Number of latitudes
set nq="128"       # Number of contours used to represent PV
set omega=$twopi   # Planetary rotation rate
set t_save="0.25"  # Coarse grid save time-interval
set t_sim="1.0"    # Fine grid residual and contour save time-interval
set n_period="10"  # Number of t_sim periods to run 
set rekman="0.0"   # Eckman damping coefficient 
set esr="0.0"      # Enstrophy injection rate
set ksr="16"       # Centroid wavenumber of enstrophy injection
set cdamp="2.0"    # Residual PV bi-harmonic hyperviscous damping rate 
                   # per day on wavenumber ng is cdamp*zz_rms:

#==========================================================================
# Choose basic physical parameters:

# Vertical:horizontal aspect ratio of surface, b:
set asp="1.0"

echo ' Planetary rotation rate, omega:     ' $omega >> job_info

# Set basic grid resolution:
echo
echo ' A longitude-latitude grid of dimensions 2*ng x ng is used.'
echo -n ' Number of latitudes, ng (default' $ng')? '
set var=$<
if ($var != "") set ng=$var

echo ' ' >> job_info
echo ' Horizontal grid resolution, ng:     ' $ng >> job_info

# Latitudinal grid spacing:
set gl=`echo "$pi / $ng" | bc -l`

#============================================================
# Set the PV jump across contours:
set dq=`echo "scale=12; 4*$omega/$nq" | bc -l`

echo ' ' >> job_info
echo ' PV jump across all contours, dq:    ' $dq >> job_info

#==========================================================================
# Set numerical parameters:

# Maximum allowed time step:
set dt_max=`echo "scale=8; $t_save / 10" | bc -l`

#============================================================
# Build parameter file with cpp and make all codes:

# Put all these dimensions into the dimens file needed for compilation:
mkdir src
cd src
cp $srcdir/* .
cp -r $hydradir/init .
cp -r $hydradir/post .

echo 
echo " Compiling source files....."
echo " ---------------------------------------------------"

# Use C pre-processor to put chosen parameters $copts into parameters.f90
set copts1="-DN_G=$ng -DN_PER=$n_period -DDT_MAX={$dt_max}d0 -DT_SAV={$t_save}d0 -DT_SIM={$t_sim}d0"
set copts2="-DPV_JUMP={$dq} -DC_DAMP={$cdamp}d0 -DASPECT={$asp}d0 -DOMEGA={$omega}d0"
set copts3="-DR_EKMAN={$rekman}d0 -DESR={$esr}d0 -DKSR=$ksr -DISEED=$iseed"
set copts=`echo $copts1 $copts2 $copts3`
precomp $copts parameters.f90

# Compile all codes and move sources to sub-directory:
make $algo setup $datagen proxy_post_all install clean

cd ..
echo " ---------------------------------------------------"
echo 

#============================================================
# Execute the data generation script:
$bindir/$datagen

#==========================================================================
# Create a directory named after the data generation script:
cd ..
if (!(-d $datagen)) mkdir $datagen
cd $datagen

# Set the job directory name:
echo
echo -n ' Job directory name (default n'{$ng}')? '
set basejobdir=$<
if ($basejobdir == "") set basejobdir=n{$ng}

# work out the last run which has been performed:
# First make a bogus empty directory so this works!
mkdir {$basejobdir}r000
set last_run = `/bin/ls -d {$basejobdir}r??? | tail -c 4 | head -c 3`
rmdir {$basejobdir}r000

@ next_run = ( $last_run + 1 )
@ p1 = ( $next_run / 100 )
@ jr = ( $next_run - ( 100 * $p1 ) )
@ p2 = ( $jr / 10 )
@ p3 = ( $jr - ( 10 * $p2 ) )
set pind={$p1}{$p2}{$p3}

set jobdir={$basejobdir}r{$pind}

#=============================================================
# Lastly move all data to desired job directory:

# Move temporary directory to job directory:
mv $tmpdir $jobdir
cd $jobdir
set datadir=`pwd`

# Copy handy script for viewing zonal averages:
/bin/cp $bindir/zonalview .

echo ' ' >> job_info
echo ' Job directory:' $datadir >> job_info

# Launch the job:
cd $datadir
bjob caps

echo ' Job launched in directory'
echo $datadir

echo
echo ' To see the running job, type'
echo ' cd' $datadir
echo ' tailf monitor.asc'
echo
